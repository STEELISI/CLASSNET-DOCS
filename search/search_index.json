{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to COMUNDA \uf0c1 The COMUNDA portal is a place for security and network researchers to access datasets shared by the CLASSNET project. Currently, several different types of datasets are shared, including: (1) datasets of hosts replying to periodic probes, (2) datasets of traffic from large academic networks, (3) datasets of traffic going to a Darknet. In addition to sharing on-going dataset collections, COMUNDA also contains curated snapshots of relevant security and network events from these datasets, with corresponding labels. Please explore our support documentation to learn how you can access the datasets and how you can discuss these datasets with the broader research community. CLASSNET project has been funded by the National Science Foundation via awards #8115780 and #1823192.","title":"Welcome to COMUNDA"},{"location":"#welcome-to-comunda","text":"The COMUNDA portal is a place for security and network researchers to access datasets shared by the CLASSNET project. Currently, several different types of datasets are shared, including: (1) datasets of hosts replying to periodic probes, (2) datasets of traffic from large academic networks, (3) datasets of traffic going to a Darknet. In addition to sharing on-going dataset collections, COMUNDA also contains curated snapshots of relevant security and network events from these datasets, with corresponding labels. Please explore our support documentation to learn how you can access the datasets and how you can discuss these datasets with the broader research community. CLASSNET project has been funded by the National Science Foundation via awards #8115780 and #1823192.","title":"Welcome to COMUNDA"},{"location":"account/","text":"Opening Account \uf0c1 COMUNDA does not support stand-alone authentication. Instead, you can choose one of three third-party authentication services: CILogon (log in with your institutional account) Github Google On the first login, COMUNDA will receive your name and email from the third-party service you used. We will also ask for your position at the research institution where you work or study, and for affiliation (name of your institution). After inputting this data, you can start using the portal.","title":"Opening Account"},{"location":"account/#opening-account","text":"COMUNDA does not support stand-alone authentication. Instead, you can choose one of three third-party authentication services: CILogon (log in with your institutional account) Github Google On the first login, COMUNDA will receive your name and email from the third-party service you used. We will also ask for your position at the research institution where you work or study, and for affiliation (name of your institution). After inputting this data, you can start using the portal.","title":"Opening Account"},{"location":"contribute/","text":"Contributing Datasets \uf0c1 If you or your organization have datasets, which you would like to distribute via our portal, please contact us via comunda-ops@googlegroups.com . Mechanisms for Sharing \uf0c1 We can host only your metadata for each dataset, or we can host your dataset and its metadata. You would provide the Data Use Agreement (DUA), outlining acceptable use of your dataset by researchers. Mechanisms for Researcher Access Approval \uf0c1 Once a researcher submits the request for a dataset, and the signed DUA, that request is routed for approval via email. You can decide who should be in charge of request approvals, e.g., someone from CLASSNET project or someone from your organization. You can also designate multiple approvers. You can further decide what is your criteria for approval, e.g., do you want to allow access only to researchers from some countries, or only to faculty but not students, etc. Once the request is approved, the researcher will receive an email with instructions how to access data. Mechanisms for Data Access \uf0c1 Currently we support three types of data access: download - researcher obtains a link to download the dataset onsite - researcher obtains login credentials to log into a machine, usually at the data provider's site, to access and process the data cloud - researcher obtains information how to access the dataset on a cloud","title":"Contributing Datasets"},{"location":"contribute/#contributing-datasets","text":"If you or your organization have datasets, which you would like to distribute via our portal, please contact us via comunda-ops@googlegroups.com .","title":"Contributing Datasets"},{"location":"contribute/#mechanisms-for-sharing","text":"We can host only your metadata for each dataset, or we can host your dataset and its metadata. You would provide the Data Use Agreement (DUA), outlining acceptable use of your dataset by researchers.","title":"Mechanisms for Sharing"},{"location":"contribute/#mechanisms-for-researcher-access-approval","text":"Once a researcher submits the request for a dataset, and the signed DUA, that request is routed for approval via email. You can decide who should be in charge of request approvals, e.g., someone from CLASSNET project or someone from your organization. You can also designate multiple approvers. You can further decide what is your criteria for approval, e.g., do you want to allow access only to researchers from some countries, or only to faculty but not students, etc. Once the request is approved, the researcher will receive an email with instructions how to access data.","title":"Mechanisms for Researcher Access Approval"},{"location":"contribute/#mechanisms-for-data-access","text":"Currently we support three types of data access: download - researcher obtains a link to download the dataset onsite - researcher obtains login credentials to log into a machine, usually at the data provider's site, to access and process the data cloud - researcher obtains information how to access the dataset on a cloud","title":"Mechanisms for Data Access"},{"location":"dataset/","text":"Interacting with Datasets \uf0c1 There are several actions you can take to interact with datasets: Search for datasets - this is enabled both for users that are not logged in and for logged in users. You can search using keywords, type of dataset and the dataset provider's organization. Search results are displayed in a short form. You can click on \"View\" at the bottom right to see expanded results. Request a dataset - if you identify a dataset of interest to your research, you can request it. Usually this takes a few days for the request to be vetted and processed. Requests for datasets from the same provider or in the same category may be expedited on our end. Review and rate the dataset - you can input positive or negative reviews and rating for any dataset, which you have previously requested. Please be respectful of data providers and use objective, professional language in your reviews. Share your labels and label corrections - some datasets will be accompanied by labels, e.g., denoting benign and malicious traffic. You can submit corrections to these labels, or even submit your own, alternative approach to labeling a given dataset.","title":"Searching, viewing and rating"},{"location":"dataset/#interacting-with-datasets","text":"There are several actions you can take to interact with datasets: Search for datasets - this is enabled both for users that are not logged in and for logged in users. You can search using keywords, type of dataset and the dataset provider's organization. Search results are displayed in a short form. You can click on \"View\" at the bottom right to see expanded results. Request a dataset - if you identify a dataset of interest to your research, you can request it. Usually this takes a few days for the request to be vetted and processed. Requests for datasets from the same provider or in the same category may be expedited on our end. Review and rate the dataset - you can input positive or negative reviews and rating for any dataset, which you have previously requested. Please be respectful of data providers and use objective, professional language in your reviews. Share your labels and label corrections - some datasets will be accompanied by labels, e.g., denoting benign and malicious traffic. You can submit corrections to these labels, or even submit your own, alternative approach to labeling a given dataset.","title":"Interacting with Datasets"},{"location":"examplesofuse/","text":"Examples of Use \uf0c1 This page provides examples how labeled datasets can be used and why having multiple labels for a dataset may benefit researchers. DDoS Hackathon 2020 \uf0c1 In 2020, we organized a hackathon at ACM Sigcomm to label DDoS attacks in FrontRange GigaPop datasets. These datasets can be requested from our portal (search for \"hackathon\"). The datasets contain two types of labels: the original labels from Peakflow appliance, and the labels proposed by USC/ISI that align start and stop time of attacks to match the rise and fall of anomalous traffic, reported by Peakflow. For example, if DNS responses to target T started to rise at 10 am, and Peakflow detected the attack at 10:05 am, USC/ISI labels would show 10 am as the attack start. Since there is no ground truth data for these attacks, it may be difficult for researchers to justify use of one set of labels over the other. Instead, we hope that researchers may benefit from reporting their results over multiple labels. You can simply read over the first three steps and then run the demo in the fourth step in Google Colab. Or, if you want to redo our steps from scratch you can request and download the hackathon dataset from our COMUNDA portal . You will also need labels for this dataset, which can be obtained from this link by cloning the repository: git clone https://github.com/STEELISI/COMUNDA.git Step 1: Select training and testing data \uf0c1 For this example we used 1 hour of data on May 12, 2020, i.e., files named nfcapd.2020051202 for training and 1 hour of data on Sep 14, 2020, i.e., files named nfcapd.2020091423 for testing. We chose to separate training and testing data in time to mimic the actual DDoS detection where past traffic and attacks inform future detection. The chosen data contains several attacks. Copy the training data into a folder called train and testing data into a folder called test . Step 2: Labeling the records using event labels \uf0c1 To label the records we ran the following commands (assuming the data and the labels were all stored in one common directory and the command is ran there as well). Note: they may take up to one hour per command, depending on your disk and CPU speed. They each produce 1.3-1.5GB files. perl tag_flows.pl train ddos_hackathon-20200511/provider-peakflow/may > train_peak_labeled.txt perl tag_flows.pl train ddos_hackathon-20200511/uscisi/may > train_uscisi_labeled.txt perl tag_flows.pl test ddos_hackathon-20200511/provider-peakflow/sep > test_peak_labeled.txt perl tag_flows.pl test ddos_hackathon-20200511/uscisi/sep > test_uscisi_labeled.txt Note that tag_flows.pl can be obtained from COMUNDA repository from folder tools/usc-isi/netflow-ddos . Step 3: Mining the features for learning \uf0c1 To mine the features for learning we ran the following commands: perl mine_features.pl train_peak_labeled.txt > train_peak.csv perl mine_features.pl test_peak_labeled.txt > test_peak.csv perl mine_features.pl train_uscisi_labeled.txt > train_uscisi.csv perl mine_features.pl test_uscisi_labeled.txt > test_uscisi.csv We then saved the csv files online, so they could be used in Google Colab. Step 4: Running ML algorithms on the data \uf0c1 To start we just ran a decision-tree algorithm using first peakflow and then uscisi labels. This example can be found on Google Colab . Since each run is sub-sampling data from our files, we ran 10 times and the table below shows our results. Labels/Run 1 2 3 4 5 6 7 8 9 10 Peakflow 0.693 0.739 0.739 0.727 0.743 0.728 0.745 0.743 0.746 0.753 USC/ISI 0.812 0.776 0.786 0.795 0.774 0.802 0.776 0.765 0.755 0.784 These results could be reported by a researcher as follows Our machine learning algorithm achieved average accuracy of 73.6% on peakflow labels and 78.2% on uscisi labels, on ddos_hackathon-20200511 dataset released by the CLASSNET project. You can also replace the decision tree with another classifier, like support vector machines. This yields the results as follows: Average accuracy of 67.6% on peakflow labels and 79.4% on uscisi labels, on ddos_hackathon-20200511 dataset released by the CLASSNET project. * The code is on Google Colab . In general, USC/ISI labels produce better accuracy, because they are better aligned with the actual traffic rise and fall during attacks.","title":"Examples of Use"},{"location":"examplesofuse/#examples-of-use","text":"This page provides examples how labeled datasets can be used and why having multiple labels for a dataset may benefit researchers.","title":"Examples of Use"},{"location":"examplesofuse/#ddos-hackathon-2020","text":"In 2020, we organized a hackathon at ACM Sigcomm to label DDoS attacks in FrontRange GigaPop datasets. These datasets can be requested from our portal (search for \"hackathon\"). The datasets contain two types of labels: the original labels from Peakflow appliance, and the labels proposed by USC/ISI that align start and stop time of attacks to match the rise and fall of anomalous traffic, reported by Peakflow. For example, if DNS responses to target T started to rise at 10 am, and Peakflow detected the attack at 10:05 am, USC/ISI labels would show 10 am as the attack start. Since there is no ground truth data for these attacks, it may be difficult for researchers to justify use of one set of labels over the other. Instead, we hope that researchers may benefit from reporting their results over multiple labels. You can simply read over the first three steps and then run the demo in the fourth step in Google Colab. Or, if you want to redo our steps from scratch you can request and download the hackathon dataset from our COMUNDA portal . You will also need labels for this dataset, which can be obtained from this link by cloning the repository: git clone https://github.com/STEELISI/COMUNDA.git","title":"DDoS Hackathon 2020"},{"location":"examplesofuse/#step-1-select-training-and-testing-data","text":"For this example we used 1 hour of data on May 12, 2020, i.e., files named nfcapd.2020051202 for training and 1 hour of data on Sep 14, 2020, i.e., files named nfcapd.2020091423 for testing. We chose to separate training and testing data in time to mimic the actual DDoS detection where past traffic and attacks inform future detection. The chosen data contains several attacks. Copy the training data into a folder called train and testing data into a folder called test .","title":"Step 1: Select training and testing data"},{"location":"examplesofuse/#step-2-labeling-the-records-using-event-labels","text":"To label the records we ran the following commands (assuming the data and the labels were all stored in one common directory and the command is ran there as well). Note: they may take up to one hour per command, depending on your disk and CPU speed. They each produce 1.3-1.5GB files. perl tag_flows.pl train ddos_hackathon-20200511/provider-peakflow/may > train_peak_labeled.txt perl tag_flows.pl train ddos_hackathon-20200511/uscisi/may > train_uscisi_labeled.txt perl tag_flows.pl test ddos_hackathon-20200511/provider-peakflow/sep > test_peak_labeled.txt perl tag_flows.pl test ddos_hackathon-20200511/uscisi/sep > test_uscisi_labeled.txt Note that tag_flows.pl can be obtained from COMUNDA repository from folder tools/usc-isi/netflow-ddos .","title":"Step 2: Labeling the records using event labels"},{"location":"examplesofuse/#step-3-mining-the-features-for-learning","text":"To mine the features for learning we ran the following commands: perl mine_features.pl train_peak_labeled.txt > train_peak.csv perl mine_features.pl test_peak_labeled.txt > test_peak.csv perl mine_features.pl train_uscisi_labeled.txt > train_uscisi.csv perl mine_features.pl test_uscisi_labeled.txt > test_uscisi.csv We then saved the csv files online, so they could be used in Google Colab.","title":"Step 3: Mining the features for learning"},{"location":"examplesofuse/#step-4-running-ml-algorithms-on-the-data","text":"To start we just ran a decision-tree algorithm using first peakflow and then uscisi labels. This example can be found on Google Colab . Since each run is sub-sampling data from our files, we ran 10 times and the table below shows our results. Labels/Run 1 2 3 4 5 6 7 8 9 10 Peakflow 0.693 0.739 0.739 0.727 0.743 0.728 0.745 0.743 0.746 0.753 USC/ISI 0.812 0.776 0.786 0.795 0.774 0.802 0.776 0.765 0.755 0.784 These results could be reported by a researcher as follows Our machine learning algorithm achieved average accuracy of 73.6% on peakflow labels and 78.2% on uscisi labels, on ddos_hackathon-20200511 dataset released by the CLASSNET project. You can also replace the decision tree with another classifier, like support vector machines. This yields the results as follows: Average accuracy of 67.6% on peakflow labels and 79.4% on uscisi labels, on ddos_hackathon-20200511 dataset released by the CLASSNET project. * The code is on Google Colab . In general, USC/ISI labels produce better accuracy, because they are better aligned with the actual traffic rise and fall during attacks.","title":"Step 4: Running ML algorithms on the data"},{"location":"glossary/","text":"This is glossary placeholder","title":"Glossary"},{"location":"labels/","text":"Viewing, Sharing and Collaborating with Dataset Labels \uf0c1 Some datasets in our collections are already labeled with security-specific and/or other relevant labels that offer insight into each record within a dataset (e.g., \"benign\", \"attack\", etc.) Researchers may want to contribute their own labels, either as an alternative to any existing labels or to create entirely new labels denoting new features of flows, hosts, records or events. Finally, if you believe that the existing labels are in error or could be improved, feel free to submit a request to update the existing set of labels as well. To add your contributions to the existing set of labels for a dataset, please follow the instructions below to share labels. Viewing the labels \uf0c1 The labels for all datasets currently exist within a single GitHub repository. To view and work with them, you will need access to the dataset itself (or else the labels will have little value) and the label repository as well. To get the label repository, download the repository from github: git clone https://github.com/STEELISI/COMUNDA.git Correcting existing labels \uf0c1 You can offer corrections to the existing labels using GitHub pull requests or by sending us a URL with your corrected labels. Creating Github Pull Requests \uf0c1 Begin by forking the main repository on github using the Fork button on the website for the COMUNDA repository: https://github.com/STEELISI/COMUNDA.git Then clone (check out) your fork: git clone https://github.com/YOURACCOUNT/COMUNDA.git Find the folder where the dataset you wish to offer corrections for is, and within that find the folder where our official labels reside Create a git branch to save your changes in: git checkout -b your-chosen-name Mark (add) any changed files you wish to submit for inclusion git add [changed-files] Commit and push git commit -m 'some message here' git push origin your-chosen-name Finally, go to COMUNDA Github and submit a pull request referencing the branch that you just pushed to your own forked copy. Make sure to provide a detailed explanation in your request about why you are suggesting the corrections you made. Sending us a URL of changes \uf0c1 Alternatively, if you are unfamiliar with using git and GitHub, you may send us a request via the COMUNDA interface. On the COMUNDA Web page log in and then click on the Label Datasets link on the left. This will enable you to provide us a URL with the materials and changes that describe your label correction. Please make sure to provide: Your reasoning for the correction A file with corrected labels and unique record IDs (ideally corrected labels would be in the format that the original labels) Share New Labels \uf0c1 For creating new labels, use the same process as described in the above instructions for correcting labels. But, instead of modifying existing files, create an entirely new folder instead for your newly created labels. Name this new folder in a way that denotes your or your research group's identity and the purpose of the labels (e.g., JMirkovic-ddos-attack). Place the following information and files into the folder: In a README.md file include: your name and contact information. The label format used in your labeling files. Make sure to explain the meaning and purpose for each label. The labels you want to contribute either in the form of a file containing the labels, or as a program or script to execute for generating the labels given the dataset files. Provide instructions and/or examples how to use the labels or run your program or script and the inputs it needs. Finally, push your forked GitHub branch to github ( git push origin your-chosen-name ) and then create a pull request at COMUNDA Github as described above. Make sure to offer a detailed explanation in your request about the reasoning for the labels you suggested and the labeling algorithm you implemented. Example scenario \uf0c1 For example, if I were to label DDoS attacks on a large dataset containing Netflow records, I may choose to have labels in the format of attack events that include the start time of attack (in UTC), the stop time of attack, the target, the type of attack or the attack traffic's signature. I would specify the format of any files containing this information in a newly created README.md file, along with any other pertinent information. I would then place the events into a label file (potentially a CSV, JSON or tab-separated format). I might also provide a program or script to read the Netflow records and create a label file that's output would include a unique record ID (for example timestamp,sourceIP,sourceport,destIP,destport) and an associated label (e.g. \"attack\" or \"ddos\"). I would put the full example of the command line needed for running my program in the README.md file. Sending us a URL \uf0c1 You may also send us a URL containing all of the information needed for a new set of dataset labels. To do so, log into the COMUNDA Web page and click on the Label Datasets link on the left-hand side. This will provide you a form to fill out with a URL pointing to your materials to download, and a textbox to please a description in that describes your proposed labels. Like the above, please provide: In a README.md file include: your name and contact information. The label format used in your labeling files. Make sure to explain the meaning and purpose for each label. The labels you want to contribute either in the form of a file containing the labels, or as a program or script to execute for generating the labels given the dataset files. Provide instructions and/or examples how to use the labels or run your program or script and the inputs it needs.","title":"Updating Labels"},{"location":"labels/#viewing-sharing-and-collaborating-with-dataset-labels","text":"Some datasets in our collections are already labeled with security-specific and/or other relevant labels that offer insight into each record within a dataset (e.g., \"benign\", \"attack\", etc.) Researchers may want to contribute their own labels, either as an alternative to any existing labels or to create entirely new labels denoting new features of flows, hosts, records or events. Finally, if you believe that the existing labels are in error or could be improved, feel free to submit a request to update the existing set of labels as well. To add your contributions to the existing set of labels for a dataset, please follow the instructions below to share labels.","title":"Viewing, Sharing and Collaborating with Dataset Labels"},{"location":"labels/#viewing-the-labels","text":"The labels for all datasets currently exist within a single GitHub repository. To view and work with them, you will need access to the dataset itself (or else the labels will have little value) and the label repository as well. To get the label repository, download the repository from github: git clone https://github.com/STEELISI/COMUNDA.git","title":"Viewing the labels"},{"location":"labels/#correcting-existing-labels","text":"You can offer corrections to the existing labels using GitHub pull requests or by sending us a URL with your corrected labels.","title":"Correcting existing labels"},{"location":"labels/#creating-github-pull-requests","text":"Begin by forking the main repository on github using the Fork button on the website for the COMUNDA repository: https://github.com/STEELISI/COMUNDA.git Then clone (check out) your fork: git clone https://github.com/YOURACCOUNT/COMUNDA.git Find the folder where the dataset you wish to offer corrections for is, and within that find the folder where our official labels reside Create a git branch to save your changes in: git checkout -b your-chosen-name Mark (add) any changed files you wish to submit for inclusion git add [changed-files] Commit and push git commit -m 'some message here' git push origin your-chosen-name Finally, go to COMUNDA Github and submit a pull request referencing the branch that you just pushed to your own forked copy. Make sure to provide a detailed explanation in your request about why you are suggesting the corrections you made.","title":"Creating Github Pull Requests"},{"location":"labels/#sending-us-a-url-of-changes","text":"Alternatively, if you are unfamiliar with using git and GitHub, you may send us a request via the COMUNDA interface. On the COMUNDA Web page log in and then click on the Label Datasets link on the left. This will enable you to provide us a URL with the materials and changes that describe your label correction. Please make sure to provide: Your reasoning for the correction A file with corrected labels and unique record IDs (ideally corrected labels would be in the format that the original labels)","title":"Sending us a URL of changes"},{"location":"labels/#share-new-labels","text":"For creating new labels, use the same process as described in the above instructions for correcting labels. But, instead of modifying existing files, create an entirely new folder instead for your newly created labels. Name this new folder in a way that denotes your or your research group's identity and the purpose of the labels (e.g., JMirkovic-ddos-attack). Place the following information and files into the folder: In a README.md file include: your name and contact information. The label format used in your labeling files. Make sure to explain the meaning and purpose for each label. The labels you want to contribute either in the form of a file containing the labels, or as a program or script to execute for generating the labels given the dataset files. Provide instructions and/or examples how to use the labels or run your program or script and the inputs it needs. Finally, push your forked GitHub branch to github ( git push origin your-chosen-name ) and then create a pull request at COMUNDA Github as described above. Make sure to offer a detailed explanation in your request about the reasoning for the labels you suggested and the labeling algorithm you implemented.","title":"Share New Labels"},{"location":"labels/#example-scenario","text":"For example, if I were to label DDoS attacks on a large dataset containing Netflow records, I may choose to have labels in the format of attack events that include the start time of attack (in UTC), the stop time of attack, the target, the type of attack or the attack traffic's signature. I would specify the format of any files containing this information in a newly created README.md file, along with any other pertinent information. I would then place the events into a label file (potentially a CSV, JSON or tab-separated format). I might also provide a program or script to read the Netflow records and create a label file that's output would include a unique record ID (for example timestamp,sourceIP,sourceport,destIP,destport) and an associated label (e.g. \"attack\" or \"ddos\"). I would put the full example of the command line needed for running my program in the README.md file.","title":"Example scenario"},{"location":"labels/#sending-us-a-url","text":"You may also send us a URL containing all of the information needed for a new set of dataset labels. To do so, log into the COMUNDA Web page and click on the Label Datasets link on the left-hand side. This will provide you a form to fill out with a URL pointing to your materials to download, and a textbox to please a description in that describes your proposed labels. Like the above, please provide: In a README.md file include: your name and contact information. The label format used in your labeling files. Make sure to explain the meaning and purpose for each label. The labels you want to contribute either in the form of a file containing the labels, or as a program or script to execute for generating the labels given the dataset files. Provide instructions and/or examples how to use the labels or run your program or script and the inputs it needs.","title":"Sending us a URL"},{"location":"request/","text":"Requesting Datasets \uf0c1 If you want to request a dataset, please click \"Request\" button at the bottom right of the Dataset view. You will be asked to fill in some information, such as who will use the dataset, for what purpose and their contact information. You will also be shown a Data Use Agreement. Filling in your information and submitting a dataset request counts as you legally accepting the Data Use Agreement. Once request is submitted it will go through our approval process, and you will receive regular email correspondence about its progress. Once you are approved, you will receive an email with directions how to access the dataset. Some datasets can be downloaded onto your machine. Some require access via Google Big Table or similar public cloud service. Some datasets can only be used at a given machine, administered by the dataset provider. The approval email will contain all the necessary information, and it will also provide a contact information that you can use if you have any questions. In some cases, a dataset you request may be part of a bigger collection of datasets, which use the same Data Use Agreement. If you were already approved to access another dataset from the same collection, your request may be expedited. In any case, the email correspondence you receive from us will guide you through the process. In some cases, your request may be declined. Since each data provider has their own criteria for approval, some providers may be very restrictive, while others may be more open. If your request is declined, you will receive an email detailing the provider's reason for this decision. Please understand that many data providers must work with their own and their customers' legal departments to provide these datasets, and that some approval criteria may seem unfair to you, but it is legally required by entities that govern the provider's business. Approval criteria may also change over time as providers learn about benefits of sharing their datasets with researchers.","title":"Requesting Datasets"},{"location":"request/#requesting-datasets","text":"If you want to request a dataset, please click \"Request\" button at the bottom right of the Dataset view. You will be asked to fill in some information, such as who will use the dataset, for what purpose and their contact information. You will also be shown a Data Use Agreement. Filling in your information and submitting a dataset request counts as you legally accepting the Data Use Agreement. Once request is submitted it will go through our approval process, and you will receive regular email correspondence about its progress. Once you are approved, you will receive an email with directions how to access the dataset. Some datasets can be downloaded onto your machine. Some require access via Google Big Table or similar public cloud service. Some datasets can only be used at a given machine, administered by the dataset provider. The approval email will contain all the necessary information, and it will also provide a contact information that you can use if you have any questions. In some cases, a dataset you request may be part of a bigger collection of datasets, which use the same Data Use Agreement. If you were already approved to access another dataset from the same collection, your request may be expedited. In any case, the email correspondence you receive from us will guide you through the process. In some cases, your request may be declined. Since each data provider has their own criteria for approval, some providers may be very restrictive, while others may be more open. If your request is declined, you will receive an email detailing the provider's reason for this decision. Please understand that many data providers must work with their own and their customers' legal departments to provide these datasets, and that some approval criteria may seem unfair to you, but it is legally required by entities that govern the provider's business. Approval criteria may also change over time as providers learn about benefits of sharing their datasets with researchers.","title":"Requesting Datasets"},{"location":"support/","text":"Support \uf0c1 For any issues, please reach out to comunda-ops@googlegroups.com","title":"Support"},{"location":"support/#support","text":"For any issues, please reach out to comunda-ops@googlegroups.com","title":"Support"}]}